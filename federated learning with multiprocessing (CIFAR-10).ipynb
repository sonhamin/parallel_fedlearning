{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3db4242230>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from utils.customloader import CustomDataset, DatasetSplit\n",
    "from utils.dataloader import get_dataloader\n",
    "from utils.train_glob import train_global_model, test_model\n",
    "\n",
    "from models.Update import LocalUpdate\n",
    "from models.Fed import FedAvg\n",
    "\n",
    "import random\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "class Args:\n",
    "    #federated arugments\n",
    "    epochs=50\n",
    "    num_users=10\n",
    "    \n",
    "    local_ep=3\n",
    "    local_bs=100\n",
    "    bs=128\n",
    "    lr=0.01\n",
    "    momentum=0.5\n",
    "\n",
    "    num_channels=1\n",
    "    num_classes=10\n",
    "    verbose='store_true'\n",
    "    seed=1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    \n",
    "\n",
    "args = Args()    \n",
    "##############SET SEEDS FOR REPRODUCIBILITY#############\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "##############~SET SEEDS FOR REPRODUCIBILITY#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataloader /  Model / Optimizer / Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "dataset_train = datasets.CIFAR10('./data/cifar', train=True, download=True, transform=transform)\n",
    "global_train_loader = DataLoader(dataset_train, batch_size=1000, shuffle=True)\n",
    "\n",
    "dataset_test = datasets.CIFAR10('./data/cifar', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(dataset_test, batch_size=1000, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CNNCifar(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CNNCifar, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, args.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_glob = CNNCifar(args=args).to(args.device)\n",
    "net_glob.train()\n",
    "\n",
    "optimizer = optim.SGD(net_glob.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "sloss = F.cross_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_globalnet = copy.deepcopy(net_glob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Distribution of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 5000]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(global_train_loader.dataset.targets, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)\n",
    "sorted_y = copy.deepcopy(global_train_loader.dataset.targets)\n",
    "sorted_index_y = np.argsort(np.squeeze(sorted_y))\n",
    "\n",
    "class_dist=[]\n",
    "\n",
    "for i in range(args.num_classes):\n",
    "    print(i)\n",
    "    class_dist.append(np.array(sorted_index_y[sum(counts[:i]):sum(counts[:i+1])], dtype=np.int64))\n",
    "    \n",
    "non_iid = np.array(class_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual = []\n",
    "for j in range(10):\n",
    "    individual.append(np.array_split(class_dist[j], 10))\n",
    "\n",
    "user_dist=[]\n",
    "for i in range(10):\n",
    "    temp=[]\n",
    "    for j in range(10):\n",
    "        temp.append(individual[j][i])\n",
    "        \n",
    "    \n",
    "    user_dist.append((np.concatenate(temp)).astype(np.int64))    \n",
    "    \n",
    "iid=np.array(user_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.multiprocessing import multi_train_local_dif\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torchvision import datasets, transforms\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distribution = iid\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    mp.set_start_method('fork', force=True)\n",
    "    torch.set_num_threads(1)\n",
    "    \n",
    "    checkpoint_globalnet11 = copy.deepcopy(net_glob)\n",
    "    \n",
    "    for i in range(args.epochs):\n",
    "        \n",
    "        print('--------------------------------------------')\n",
    "        print(\"\\n\\n\\nstart training epoch : \" + str(i) + \"\\n\\n\\n\")\n",
    "        print('--------------------------------------------')\n",
    "        \n",
    "        procs=[]\n",
    "        loss_locals=[]\n",
    "        w_locals=[]        \n",
    "        \n",
    "        q_l = mp.Queue()\n",
    "        q_w = mp.Queue()        \n",
    "        \n",
    "        for i in range(args.num_users):\n",
    "\n",
    "            p = mp.Process(target=multi_train_local_dif, args=(q_l, q_w, args, \n",
    "                                                               i, sloss, global_train_loader, \n",
    "                                                               distribution, checkpoint_globalnet11))\n",
    "            procs.append(p)\n",
    "            p.start()\n",
    "\n",
    "        for p in procs:\n",
    "            loss_locals.append(q_l.get(p))\n",
    "            w_locals.append(q_w.get(p))\n",
    "\n",
    "        for p in procs:\n",
    "            p.join()\n",
    "\n",
    "            \n",
    "        print('--------------------------------------------\\n\\n')\n",
    "        w_glob = FedAvg(w_locals)\n",
    "        checkpoint_globalnet11.load_state_dict(w_glob)\n",
    "        test_model(checkpoint_globalnet11, test_loader, sloss, args)\n",
    "        print('\\n\\n--------------------------------------------')\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Federated Learning\n",
      "\n",
      "Test set: Average loss: 2.30375 \n",
      "Accuracy: 943/10000 (9.43%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#After fedlearning\n",
    "print('Before Federated Learning')\n",
    "test_model(net_glob, test_loader, sloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Federated Learning -- checkpoint\n",
      "\n",
      "Test set: Average loss: 1.13745 \n",
      "Accuracy: 6007/10000 (60.07%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#After fedlearning\n",
    "print('After Federated Learning -- checkpoint')\n",
    "test_model(checkpoint_globalnet11, test_loader, sloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
